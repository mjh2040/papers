### Title
What do Neural Machine Translation Models Learn about Morphology?
### Authors
Yonatan Belinkov
Nadir Durrani
Fahim Dalvi
Hassan Sajjad
James Glass

### link
[arxiv](https://arxiv.org/pdf/1704.03471.pdf)

### Contents
- cho et al(2014)이후로 seq2seq model은 NMT(Neural Machine Translation)의 거의 정석적 모형
- 그러나 encoder&decoder들의 세부적인 작동 원리에 대해서는 아직 많은 연구가 이루어지지 않음
- 본 연구에서는 다양한 방법을 통해서 3가지 점에 대해 대답하려 함

<ol>Which parts of the NMT architecture capture word structure?</ol>
<ol>What is the ivision of labor between different components?</ol>
<ol>How do different word representations help learn better morphology and modeling of infrequent words?</ol>
<ol>How do different word representations help learn better morphology and modeling of infrequent words?</ol>

- 이걸 알아내기 위해서 NMT 먼저 다양한 corpus에 대해 학습한 후 encoder이용해서 feature만 뽑아냄. 이걸 가지고 다시 다양한 분석기 만든 후 분석기 성능 비교

#### 결과
- character-based rnn이 morphology 학습에는 더 유리, 이건 너무 당연한게 morphology가 바뀌면 character 조합이 바뀌니까.. 이걸 바로 modeling할 수 있는 char-rnn이 더 유리하겠지
- 깊은 층의 feature가 단어 의미 더 많이 가지고 있음
- morphologically poor language가 translation 결과가 더 좋음.. 한국어는 역시 어려워
- attentional decoder에는 morphology 관련 정보는 별로 없음

morphologically rich language는 답이 뭔가.. bpe도 못쓰겠고 답이 없네