### Title
Batch Normalization: Accelerating Deep Network Traning by Reducing Internal Covariate Shift
### Authors
Sergey Ioffe, Christian Szegedy
### link
[arxiv](https://arxiv.org/pdf/1502.03167.pdf)

### Contents
- neural network 학습 빠르게 하는 테크닉.
- 요즘 이거 안쓰면 바보된다고 함. tensorflow, pytorch, theano 전부 다 구현되어 있음
- rnn의 layer normalization 읽기 전에 기본적인 내용 다시 한 번 보자는 의미에서 읽어봄
- covariance shift는 change of input variable distribution을 의미하는 개념. domain adaptation에서 생김(이건 당연, 학습 안시킨 data 일반화해서 적용하려는게 da니까)
- 저자들은 이 문제를 좀 더 작은 범위로 가지고 옴. mini-batch학습을 하면 전체 data를 다 쓰는게 아니기 때문에 batch마다 input distribution이 조금씩 다르게 됨
- nn은 이 변화때문에 학습이 느려짐
- 간단하게는 global mean, global variance로 normalizing하면 되는거 아님? 이렇게 생각할 수 있음. 근데 그러면 affine transformation x_hat은 x(batch sample), X(all samples) 두 변수의 함수가 되기 때문에 X의 jacobian도 필요하고 계속 메모리에 가지고 있어야 함. 이는 바보같은 짓
- 해결책은 batch statistic으로 normalizing하는 것. 그리고 x의 각 dimension마다 개별적으로 함. covariance matrix 안구해도 sample variance만 구해도 됨
- 그리고 gamma, beta로 affine transformation해서 각 차원의 특성을 유지하는 데에 도움을 줌
- inference 때에는 한 번 batch를 돌면서 unbiased estimator 구한 다음에 이를 계속 batch normalization의 statistic으로 활용함


### 유의깊게 본 점
- 이렇게 간단한 아이디어가 2015년에 나왔다는게 의외
- 그리고 이게 거의 모든 regularization 기법들을 몰아내고 다 쓰인다는 점..
- 이런 연구를 해야 되는뎈ㅋㅋㅋㅋ
- layer normalization은 어떤 차이가 있는지 잘 살펴봐야겠당