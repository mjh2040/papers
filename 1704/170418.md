### Title
HyperNetworks
### Authors
David Ha∗, Andrew Dai, Quoc V. Le
### link
[arxiv](https://arxiv.org/pdf/1609.09106.pdf)

### Contents
- hyperparameter처럼 hyper neural network 존재해서 main network parameter 예측
- cnn(fully independent parameter) <----hypernetwork----> rnn(all parameters are shared)
- 어느 쪽에도 적용 가능한 모형
- cnn은 flexibility를 좀 줄이는 대신 shared variable통해서 추정할 파라미터 수 줄어드는 반면 rnn에서는 flexibility를 더하는 대신(timepoint마다 다른 weight 사용 가능) memory problem 생길 수 있음


### 유의깊게 본 점
- cnn에서 n_input별로 kernel parameter 따로 구해서 나중에 합친 것처럼 rnn에서도 RNh×Nx×Nz 이 파라미터 한 번에 다뤄서 메모리 터지는 일이 없도록 W_h는 공유하되 이를 embedding value z에 따라서 scaling하여서 timestep마다 다른 값을 가질 수 있도록 한 점이 흥미로웠음
- rnn에서 batch normalization, layer normalization 하는 법 아직 잘 모르겠어서 논문 참고해야 할 듯(Cooij- mans et al., 2016, Ba et al., 2016)
- table 4를 보면 parameter 수는 거의 차이 안나는데 성능은 훨씬 좋음. chatbot 만들 때에 넣으면 효과가 좋으려나?


### 참고자료
[blog](http://blog.otoro.net/2016/09/28/hyper-networks/)<br/>
[github](https://github.com/hardmaru/supercell/blob/master/supercell.py)<br/>
[tensorflow_with_latest_papers](https://github.com/NickShahML/tensorflow_with_latest_papers)<br/>