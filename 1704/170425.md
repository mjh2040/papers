### Title
Feature Selection: A Data Perspective
### Authors


### link
[arxiv](http://arxiv.org/abs/1601.07996)<br>
[web repo](http://featureselection.asu.edu/)


### Contents
- feature extraction vs feature selection
- raw data의 dynamics를 간직한다는 점에서 feature selection을 더 많이 써야하는 분야들 있음(genetics, social science)

#### traditional perspective
- label 정보 이용 가능한지에 따라 supervised, unsupervised, semi-supervised
    * supervised: class label과의 correlation 이용
    * unsupervised: data similarity, local discriminative information
    
- search strategy 관점에서 wrapper, filter
    * wrapper methods: 1) search for a subset of features -> 2) evaluate the selected features -> 3) repeat until the criterion is met 2^n가지 가능성 있음
    * filter method: 1) calculate feature importance -> 2) filtering out low-rank features
    * embedded method: lasso같이 분석 과정에 feature selection 들어있는거
  
#### A data perspective
- streaming data and features: data수, feature수가 이제 고정된게 아님. 
- heterogeneous data: multi sources
- structures between features: data 사이의 structure(tree, graph, group...)


#### Feature Selection on Generic Data
- similarity based methods
    *자료 간 similarity를 가장 잘 보존할 수 있는 방식으로, data similarity matrix와 feature vector 곱해서 값이 가장 크도록 하는 것이 거의 모든 방식의 기본
- information theoretical based methods
    * most algorithms are performed in a supervised way
    * I(X; Y): information gain 이게 중요한 컨셉. 결국 Y를 알 때 X에 대한 정보가 많은 자료가 좋은 자료인거지(Y랑 많이 관련이 된 자료이니까)
- sparse learning based methods
    * algorithm 내에 변수 선택 과정 포함, 요즘 트렌드
    * regularization term을 어떤걸 사용하는지 중요
    * 여기서도 요즘은 unsupervised 방법을 쓰려고 함. MCFS같은 경우는 top-k largest eigenvalue에 해당하는 eigenvector들을 구한 후 여기에 대해 회귀분석을 해서 유의미한 변수를 찾으려고 함. 이 말인 즉슨 중요한 변수(variance를 많이 설명할 수 있는)를 만들어내는 데에 중요하게 쓰이는 변수들을 찾아서 사용하겠다는 뜻
- statistical based methods
    * 단순 통계치들을 이용해서 걸러냄(low variance, t,f-scores 등등)

#### Feature Selection with Structure Features
1) group structure
- w = argmin*loss*(W:X,y) + $/alpha$/*penalty*(w, G), where G denotes structures among features (group lasso)

2) tree structure
- tree-guided group lasso: 

3) graph structure
- graph lasso, GFlasso
- graph structure의 기본 아이디어는 connected feature는 비슷한 coefficient 가져야 한다는 것
- min loss(w;X,y)+α||w||1 +(1−α)A(i, j)(wi −wj)^2 여기에서 A(i, j)값이 크면(connected) wi, wj가 같아야 페널티를 덜먹게 됨

#### Feature Selection with Heterogeneous Data
- 3가지 경우 나눠서, linked data; multi-source; multi-view
1) feature selection on networks
    * supervised way: min||WX-YW|| + XW*X'W'의 trace로 penalty를 주는데 L(link)로 가중치를 줘서..
    * unsupervised way: link data랑 feature를 가지고 latent features(pseudo-class)를 찾아내는 느낌으로.. 자세한건 논문을 봐야 알 듯..
2) multi-source data: multiple source에서 covariance matrix를 계산하는 것이 핵심
3) multi-view data: Sparse group lasso 읽어보기

#### Streaming data
- twitter같이 실시간으로 text stream이 들어오는 경우
- 대체로 2단계임. 1> 새로 들어온 feature를 받아들일지 2> 이미 있는 feature중 버릴 것이 있는지
1) grafting: |*d_loss*(W;X,y)/*d*_w| > alpha(threshold)인 경우에만 새로운 feature f를 받아들임 -> 새로운 요소 받아들여서 loss가 충분히 줄어드는지를 보는거
2) online streaming feature selection: 이건 2단계. 먼저 그럴싸한거 다 골라내고 그 다음 약한거 다 쳐내고


