### Title
Layer Normalization
### Authors
Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton
### link
[arxiv](https://arxiv.org/pdf/1607.06450.pdf)

### Contents
- Layer normalization은 batch normalization의 rnn 적용버전
- batch normalization은 각 batch마다 분포가 다르다는 점이 문제였음(covariate shift)
- 각 층마다 input을 batch의 평균, 표준편차로 표준화를 해서 batch마다 input이 다르다는 점, layer마다 다른 input을 받는다는 점에서 오는 문제점들을 극복함
- rnn같은 경우 위처럼 하기가 힘들어. 일단 구조가 recurrent한 것도 있고 자료 길이도 다 다름
- rnn에선 그럼 어떻게 하느냐? 저자들이 제안한 방식은 무척 간단함
- 원래 rnn의 input shape는 이렇게 생겼음 [N, T, D] where N: batch size, T: timestep, D: input dimension
- Time step을 앞으로 보내면 [T, N, D]이고 timestep별로 [N, D] matrix가 들어있는 꼴 -> batch normalization과 사실 비슷하게 생김
- batch normalization은 [N, D] -> [1, D] (mean, variance)를 구해서 이 값들로 표준화를 했다면 layer normalization에서는 [N, D] -> [N, 1] 즉 output hidden값의 평균, 표준편차를 구해서 그 값으로 각 자료들을 표준화를 함
- batch normalization에서는 inference 단계에서 자료 1개 표준편차를 못구하니까 sample statistic을 구해서, 혹은 moving average statistic을 구해서 사용했다면 layer normalization에서는 그런 문제도 없음. lstm layer의 timestep별 output이 128 dim이라면 이 값을 가지고 평균, 표준편차 구해서 표준화하면 됨

### 유의깊게 본 점
- 굉장히 간단한 응용인데도 효과가 좋다
- 공대 논문스럽지 않게 수학적 증명이 꽤 충실하다
- tensorflow의 contrib에 이미 layer normalization이 구현되어 있음. 사용하기 쉬우니 내 모형에 바로 넣어보자