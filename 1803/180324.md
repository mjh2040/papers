### Title
On the Importance of Single Directions for Generalization

### Authors
Ari S. Morcos, David G.T. Barrett, Neil C. Rabinowitz, & Matthew Botvinick

### link
[Download link](https://arxiv.org/pdf/1803.06959.pdf)

### Contents
- 왜 신경망은 전체 데이터를 몽땅 다 외워버릴 정도로 flexible하면서도 generalization이 잘되는걸까?
- !image](https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Overfitted_Data.png/300px-Overfitted_Data.png)
- 다양한 가설들
    - flatness of minima
    - sharp minima?
    - SGD
    - ...
- 여기서 실험해본 것
    1. measure the importence of single direction
        - 하나의 neuron 값 / feature map에서의 하나의 cell의 값을 고정
        - model performance가 어떻게 달라지는지 알아봄
    2. addition of noise
        - add gaussian noise to all units
- selectivity index
    - selectivity = (\mu\_max - \mu\_-max) / (\mu\_max - \mu\_-max)
    - mu_max : highest class-contidional mean activity
    - mu_-max: mean activity across all other class
    - 어떤 뉴런이 특정 클래스에서만 강하게 발화하는지 알아보기 위한거
- 왜 이런 single direction을 찾으려고 했을까?
    - 만일 데이터 전부를 외워버리는 network가 있다면 generalization은 잘안될거임
    - 그리고 이런 network는 single direction에 민감할거고(아마도) 그러면 이거 하나 작살내는게 아마 성능에 큰 차이를 보일거
    - grandmother cell theory를 떠올리게 하네...

- 실험
    - dataset에 일부 label 섞어버린 애들 넣고 학습시킴
        - 이렇게 하면 dataset의 structure를 알기 힘들기 때문에 걍 답을 외워버릴거
        - 그러면 filter ablation에도 더 민감하게 반응하지 않을까?
        - 실제로 그런 경향이 나타남, noise정도에 따라 더 민감하게 반응
        - 그리고 # of filters ablated 증가하면 test accuracy가 더 빨리 떨어짐

- single direction 출현을 model selection같은 데에다가 적용할 수 있지 않을까?
    - data corruption과는 무관하게 그냥 single diretion이 많다는건 generalization이 잘안되는 징조로 볼 수 있음
    - single direction perturbation에 민감하게 반응하는 순간을 early stopping 적용 시간으로 잡아버리면?
    - 실험해보고 결과보니.. 괜찮은 지표로 써먹을 수 있을 것 같더라... 가 결론

- selectivity와 impact of ablation on loss를 보니..
    - 예상과는 반대로 selectivity와 loss 증가량 사이에는 별 관계는 없었음. 오히려 negative corr가 발견되기도..
    - 이건 왜그런걸까?

