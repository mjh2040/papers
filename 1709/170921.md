### Title
Reinforcement Learning Based Conversational Search Assistant

### Authors
Milan Aggarwal, Aarushi Arora, Shagun Sodhani, Balaji Krishnamurthy

### link
[Download link](https://arxiv.org/pdf/1709.05638.pdf)

### Contents
- Agent action space
    - all posible actions
    - probe intent actions: 사용자에게 자신의 상태를 더 알게 해주는 쿼리
    - general actions: 인사, 카트에 저장 등의 e-commerce와 관련된 포괄적 행동
    - reward는 사용자가 agent의 요청에 따랐는지, 혹은 feedback을 요청했다면 feedback점수에 따라서 결정
    
- state space
    - the set of conversation states observed by the agent
    - history of user actions, history of agent actions, discretized relevance scores of search results, length_conv
    - history는 마지막 k개의 대화
    - user쪽은 길이 7의(unique action이 7개), agent쪽은 길이 12의(unique action이 12개) onehot vector로 나타내고 이거 10개를 concatenation해서 history vector로
    - search result score는 가장 최근 쿼리와 그 쿼리에 해당하는 상품 사이의 similarity라는데.. 무슨 뜻인지 잘 이해가 안가네
    
- reward
    - total reward를 구성하는 세 요소: task completion(구매/다운로드 여부), 매 시점마다 측정한 user의 반응의 적당함, 얼마나 빨리 구매 행동에 도달하는지 여부
    
- user model
    - 실제 대화하며 구매한 데이터가 없으니 user model을 만들어서 simulation함
    - user model의 policy는 conditional probability P(user action u | history h of user actions)
    - 이 확률은 20일간의 쿼리/세션 데이터로부터 계산됨
    - user action space: {new query, refine query, request more, click result, add to cart, cluster actetory click, search similar}

- NLU engine은 자연어 입력을 query로 변환
- 검색 결과와 user action이 rl agent에게 전달
- 주어진 상태에 대해 policy에 따라 행동 결정
- 어떻게 학습시켰는지에 관한 implementation detail이 없네
- 보기엔 q-learning사용한 듯
- policy는 뭘로 모델링한거지? 왜 논문을 쓰다가 만 것이지? 그냥 grid search한거임?
- 뭔가 싸다 만 것 같은 느낌이다...