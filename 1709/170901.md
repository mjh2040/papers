### Title
Which Encoding is the Best for Text Classification in CJK?

### Authors
Xiang Zhang, Yann LeCun

### link
[Download link](http://arxiv.org/abs/1708.02657)

### Contents
1. Introduction
    - NLP 세계에서 한국어, 일본어, 중국어는 좀 차별화된 특징이 있음
    - character 무지하게 많고(일본어, 중국어는 한자때문, 한글은 조합형이 아니라 완성형으로 치면 많음, 11000자 넘지?)
    - 또한 단어 segmentation이 어려움
        - 일본어, 중국어는 띄어쓰기도 잘없고 한국어도 형태소 분석이 까다로움
    - character level cnn같은거 요새 많이 적용을 하는데 character 차원이 너무 크다보니 바로 적용이 어려움
        - one hot encoding같은거 하려고 치면 10000차원이 넘어버리니...
    - 어떤 인코딩 방법을 사용하는 것이 가장 좋을까?
    - 여기서 연구한 방법
        - 글자 대신 character image 사용(glyph image)
        - utf8 byte encoding
        - romanization
    - 어떤 기법을 사용하는 것이 가장 좋은 모형을 만들 수 있을지 한 번 연구해보겠다
    
1. Encoding Mechanisms for Convolutional Networks
    - classifier는 zhang 2015의 형태를 유지
    - character cnn 모형
    - 단어단위로 인코딩하는건 아니고 그냥 글 전체 cnn으로 처리
    
    - Character Glyph
        - 한자권 언어들의 경우 글자 stroke가 의미를 가지고 있는 경우가 있음
        - 글자 이미지를 그냥 변수로 넣는 것이 의미가 있을 수 있음
        - unifont라는거 사용해서 글자를 16*16pixel image로 변환
        - 글 길이는 큰 네트워크에서 character 512개, 작은 네트워크에서 486개를 유지하도록... 넘으면 잘라내고 모자라면 zero padding
        - 한자의 경우에는 특히 좋을 수 있을 것 같지만 한글의 경우 character 바로 사용하는 것에 비해 실효성이 있을지 모르겠음
        
    - One-hot Encoding
        - 유니코드 상에서 보면 알지만 쭝국어, 한글이 전체중에서 차지하는 비율 남바 원, 투임
        - 따라서 일반적인 형태의 one-hot encoding은 어려움(단어 임베딩을 하는 것보다 parameter가 적다는게 character cnn의 장점 중 하난데 애초에 이러면 죽도밥도 아님)
        - 여기서는 두 가지 방법을 사용
            - byte encoding
                - utf-8로 인코딩한다고 치면 한글의 경우 각 글자는 3바이트로 나타낼 수 있으니까 2*8(1byte)가 3개 있는 모양으로 나타낼 수 있음
                - 1 바이트 단위(ex. 11011110) 이걸 변수로 사용
            - romanized byte encoding
                - 글을 영문 발음으로 변환(특히 중국어, 일본어는 애초에 입력 방식이 어렇게 되어 있으니까 이 방법은 굉장히 자연스럽게 느껴질 듯?)
                - 영어 character를 byte encoding함
                - 영어는 유니코드에서 1바이트로 인코딩되니까 one-hot encoding 차원이 막 늘어질 문제가 없음
                - 그냥 영어로 바꿔서 character rnn쓰는게 더 좋지 않을까?
            - 문서 길이는 큰 네트워크는 2048(글지로 치면 4로 나눈다고 쳤을 때(utf8이 4바이트 사용하니까) 512로 위의 조건과 동일하게 맞추려고 노력한 듯
    - embedding
        - 임베딩 벡터의 output dim은 256
        - vocab size는 byte enoding은 257: 아마 2*8 + oov일 듯, romanized version도 마찬가지
        - character level encoding은 65537
        - word level은 2000002

1. Linear Models and fastText
    - classifier로 cnn말고도 linear model(multinomial logistic, fastText) 사용
    - 로지스틱의 경우 1-gram, 5-gram(word, character)을 변수로 사용
    - fastText는 feature를 마찬가지로 word, character 사용했고, (1,2,5)-gram을 변수로 넣었음
    
1. Dataset
    - 자기들이 크롤링해서 만들었고 공개해준대
    - 한글 자료는 11번가에서 긁어서 1,2,3점은 neg, 4,5는 pos로 만들어서 감정분석하는 문제

1. Word Segmentation and Romanization
    - 한글은 konlpy의 mecab으로 형태소분석했고 hangul-romanize 패키지 써서 romanization
    - 그래도 좀 알아보고 했나보네?

1. Results
    - 결과를 살펴보면 가장 좋았던건 대부분의 언어에서 character 5-gram fastText 모형
    - 근데 5-gram보다 적으면 결과가 확 안좋았음
    - convnet은 대체로 결과값이 안정적으로 좋은 편이었는데 byte-level input이 좋았음
    - 어떤 하나의 모형이 계속 좋은건 아니고 이것저것 해볼 필요성이 있었음
    - generalization gap측면에서 word, character 불문하고 n-gram 사용한게 오버피팅되는 경향 보임
    - 일단 이것들은 변수가 너무 많고 sparse하니까.. 그럴 수밖에 없을 듯
    - 얘네들이 내린 결론은 일반적인 linear model classifier에는 n-gram 방식이, character cnn 모형에는 byte-level encoding이 좋더라는거
    - cnn이 이런 낮은 차원의 변수들을 처리할 능력이 있다는 점이 놀라움
    - 개선할 여지가 있을 것 같은데 한글은 3byte씩 묶에서 이걸 embedding하면 안되나? 영어나 이런건 모자라니까 padding을 해야겠지만
    - Character-Aware Neural Language Models을 생각해보면 character embedding할 때에 character는 one-hot encoding을 썼는데 byte encoding이 훨씬 나을 것 같은 느낌이 든다
        