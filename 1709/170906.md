### Title
Adversarial Examplesfor Evaluating Reading Comprehension Systems

### Authors
Robin Jia, Percy Liang

### link
[Download link](https://arxiv.org/pdf/1707.07328.pdf)

### Contents
1. Introduction
    - 요새 적당히 테스트 자료에 대해 prediction error만 구하고 넘어가는 수가 있는데 내가 가만 안놔둠
    - 엄선된 adversarial data를 이용해서 이것도 잘하는지 볼거임
    - SQuAD reading comprehension task
    - 여기서는 distracting sentence를 문서에 추가하는 것으로 자료 만들었음
    - 어떤 룰을 사용하여 헷갈리는 문장을 선택했는지 주의깊게 봐야겠네

1. The SQuAD Task and Models
    1. Task
        - 위키피디아 한 paragraph 읽고 문제에 답하는 과제
    1. Models
        - 두 모형 썼음, BiDAF, Match-LSTM(모델에 대해서는 있다가 다시 한 번 살펴보자)
        - 그 이외에도 몇 가지 공개된 모형을 써서 test error 계산했음
    1. Standard Evaluation
        - accuracy는 예측 answer와 true answer의 F1 score로 계산함

1. Adversarial Evaluation
    - adversary function : (p, q, a) -> (p', q', a')
    - 저렇게 생성된 자료가 만족해야 되는게 두 가지 있음
        - 사람이 p'를 봤을 때에 q'라고 응답할 수 있어야 함
        - adversarial example은 기존의 것과 유사해야 함. 너무 뜬금없는 제시문으로 바뀌어도 안되고 질문이 너무 거지같은 걸로 바뀌어도 곤란
    - semantics-preserving adversaries
        - 이미지 적대적 샘플 생성에는 눈에 안띄는 노이즈 섞는 방법 사용
        - 텍스트에서는 paraphrasing에 해당 -> 근데 이건 의미를 너무 많이 바꿔버리는 경우가 많음
    - Concatenative adversaries
        - 그래서 여기서 선택한 방법은 p(제시문) + s(perturbation sentence)
        - question / answer는 바꾸지 않음
        - ADDSENT
            - 질문에서 명사, 형용사를 antonym으로 바꿔치기
            - named entity도 비슷한걸로 바꿔치기
            - 가짜 정답 만들기(정답 종류는 미리 카테고리 26개로 분류해두고 이에 맞춰서), 예시로 나와있는건 장소를 Prague -> Chicago로 바꿨음
            - 마지막에 붙일 응답으로 바꾸기 위해서 질문, 정답 결합해서 declarative form으로 변환
            - 만들어진 문장 문법 틀린거 고치기 -> 이렇게 해서 제일 마지막에 붙였음
        - ADDANY
            - 문법이라 이런거 신경 안쓰고 헷갈리게 만드는 단어 n개 추가
            - 자기들은 10개로 실험
            - common english word에서 단어 랜덤하게 뽑기
            - i번째 단어를 20개의 빈출단어와 question에 나오는 단어로 바꿔치기
            - 정답에 대한 분포가 나오면 expected f1값 구해서 이걸 최소화하도록 단어 바꿔치기 수정
            - 이건 이미지에서 loss gradient방향 따라가는 것과 비슷한 논리네

1. Experiments
    - main experiment는 BiDAF, Match-rnn의 adversarial F1값 보는거.. 70~80정도였던 값이 2~50정도로 확 떨어짐
    - addany가 더 효과가 좋네
    - 단지 자주 나오는 단어만 뒤에 붙여도 성능이 확 떨어져버림
    - 사람한테 평가하게 했을 때에도 F1값은 좀 떨어지긴 했는데 이정도는 있을 수 있는 에러라고 저자들은 이야기 함(단순 랜덤에러)
    - 오류를 분석해보니까 응답 길이가 굉장히 늘어난 경우를 볼 수 있었음. 아마도 정답 문장과 distraction 문장을 같이 묶어버려서 그런 것 같음
    - 그리고 대부분의 오답은 적대적 문장에서 튀어나옴
    - 그러면 이 샘플들도 넣어서 학습시키면 되지 않냐?
        - 그렇게 해봤음. 근데 끝에 붙인거 학습에 넣고 가운데 넣은거 테스트하니까 말짱 도루묵이었음. 즉 마지막 문장을 안보도록 학습이 된거. 의미적으로 알게된건 아니고
        - 쉽지 않다..
