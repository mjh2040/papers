### Title
Fully Character-Level Neural Machine Translation without Explicit Segmentation

### Authors
Jason Lee, Kyunghyun Cho, Thomas hofmann

### link
[Download link](https://arxiv.org/pdf/1610.03017.pdf)

### Contents
1. Introduction
    - word-level NMT가 잘 작동하고 있긴 하지만 드문 단어, OOV 같은거 처리하는건 아직도 여전히 문제임, 단어 사이즈 증가하면 계속 문제..
    - encoding, decoding에서 완전 character단위 학습
    - character-level model의 장점? multilingual translation에 유용.. 이것도 한 번 증명해보이려고 함

1. Fully Character-Level Translation
    - why character level?
        - chung et al.(2016)에서 나온 이야기는 3개
            - OOV 문제에서 자유로움
            - 단어 변용형 모형화하기 좋음(변용형 각각을 따로 단어로 안잡아도 별문제없으니)
            - segmentation 필요없음
        - 저자들이 생각하는 추가적인 이득 2가지
            - multilingual setting에 더 유리함, 왜냐면 유럽 언어의 경우 많은 알파벳들이 겹치니까
            - 모형 스스로 word boundary에 대한 정보 학습하는게 가능
    - related work
        - 요즘은 word unit보다는 subword unit을 써서 문제들을 많이 극복함
        - Costa-Jussa & Fonollosa(2016)의 접근방식은 단어레벨로 쪼개고 각 단어의 embedding matrix를 cnn으로 대체했었음
        - Luong & Manning(2016)에서는 아예 character level LSTM, attention over each character로 번역기 구현했고 나름 성능은 괜찮았지만 학습에 3달 걸림...
        - Chung et al.(2016)은 bpe단위 encoder와 character level decoder 사용했음. 이 논문은 여기서 encoder도 character-level로 바꿔버린거
    - challenges
        - character level로 하면 자료 길이가 6~8배정도 길어짐. 이걸 어떻게 통제할까?
        - decoder에서 softmax는 그렇게 시간 오래 안걸림(||y||가 적으니 normalizing term 계산하는 것도 시간도 줄테고 그러니..), 문제는 attention 부분인데 이건 계산량이 엄청 늘어버림.. 모든 token에 대해 attention 계산하려면..
        - character sequence를 어떻게 단어처럼 의미있는 정보로 표상할건지?
        - encoder에서 long range dependency를 더 신경써야 함.. lstm같은거 쓴다면 어떻게 과거 정보를 유지할 수 있을지?

1. Fully Character-Level NMT
    - 이건 구조 자체는 간단함
        1. character embedding matrix를 통해 one-hot character vector를 embedding vector로 변환
        1. character sequence(데이터 차원은 e(embedding dimension) * (T(문장 길이) + num_padding - 1)) 이렇게 되겠지)에 대해 convolution, convolution filter를 여러개를 적용해서 나온 결과값들을 stacking, 최종 output의 dimension은 e * T -> n(number of filter) * T가 되겠지
        1. 위에서 나온 output을 pooling over time으로 정보 축약하기, 이건 일종의 character n-gram의 요약된 정보를 나타내는 값이라고 볼 수 있음, 여기서는 stride를 5로 삼았는데 그려면 사실 character n+5-1 gram의 정보와 비슷한 느낌이 되는거임, 근데 생각해보면 predictable같은 단어에서 predict가 핵심 정보니까 여기서 pooling을 하면 그냥 더 긴 n-gram을 사용하는 것보다 더 의미있는 정보가 뽑혀나올 수 있다고 볼 수 있겠지..
        1. 그 다음 highway network 사용해서 넣고 recurrent layer에다가 넣어서 나온 최종 input들이 encoder states가 되는거임
        1. attention이랑 decoder는 별로 다를게 없군.. 결국 이게 이전 논문이랑 다른건 bpe대신 cnn을 사용해서 character 속에서 자동으로 의미있는 feature들을 뽑아내려고 했다는거

1. Experiment Settings
    - bpe2bpe와 bpe2char와 비교
    - WMT 15의 de-en, cs-en, fi-en, ru-en과제들
    - multilingual model의 경우 many-to-one task 수행했음
    - 근데 filter를 무지많이 사용했네.. 저러면 세로로 쌓이는 자료도 엄청나게 많았겠네.. seq2seq의 embedding dimension이 보통 128에 많아야 1024정도인걸 생각하면 여기서 나오는 결과(대략 2000~2400정도)는 정말 엄청나게 많다고 느껴짐..

1. Quantitative Analysis
    - BLEU값을 보면 거의 모든 모형에서 character level encoder를 사용한게 더 결과가 좋았음
    - 사람 평가도 비슷한 패턴..
    - 흠터레스팅.. 생각해보면 기존 연구와 크게 다를게 없다고 느껴지기도 하기는 함..
    - 논문에서 핵심은 그건거같음.. 일종의 자동화된 segmentation(다양한 길이의 cnn에서 나온 결과와 max-pooling으로 만들어지는 알아서 잘 나눠진 단위의 의미있는 표상)이 가능하다는거..
    - 아마 저 convolution값을 번역하지 말고 원래 언어로 바꿔본다면 굉장히 재밌을 것 같음.. image에서 각 conv layer가 어떤 feature들을 잡아내는지 보는 것처럼 어떤 정보들이 과연 뽑혀나온 것일지?