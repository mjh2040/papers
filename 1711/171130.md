### Title
Adversarial Feature matching for Text Generation

### Authors
Yizhe Zhang, Zhe Gan, Kai Fan, Zhi Chen, Ricardo Henao, Dinghan Shen, Lawrence Carin

### link
[Download link](https://arxiv.org/pdf/1706.03850.pdf)

### Contents
1. Introduction
    - 일반적으로 seq2seq model에서 sentence encoder를 학습을 시키면 latent space상에서 극히 적은 공간만이 커버가 됨. 나머지는 무슨 의미있는 정보를 담고 있다고 보기 힘든, 그래서 decoding을 하면 unrealistic sentence가 생성이 되어버림
    - 또한 학습시에는 ground truth 단어들로부터 정보를 받지만 실제 inference에서는 예측된 단어를 사용하기 때문에 exposure bias일어남
    - scheduled sampling(학습시에 임의로 ground truth대신 생성된 값 집어넣어서 학습하기)하면 문제가 좀 나아진다고는 하지만 학습이 불안정해짐
    - decoding해서 좋은 문장이 만들어질 수 있는 random feature를 어떻게 하면 latent space로부터 잘 뽑아낼 수 있을까?
    - 이 논문에서는 GAN을 사용함. 기본적인 논리는 다음과 같음
    - 만일 내가 임의로 생성한 값이 실제 문장을 encoding했을 때와 유사하다면(GAN의 discriminator가 구분을 잘못한다면) 내가 생성해 낸 새로운 feature는 좋은 문장을 만들어낼 수 있는 새로운 feature vector가 아니겠느냐?

1. Model
    - GAN model에서 Generator는 LSTM(latent feature z를 받아서 문장 생성하는 모형), Discriminator가 구분해야 할 feature 만드는 부분은 CNN(생성된 문장 cnn으로 처리해서 비교)이용
    - cnn으로 문장 encoding -> z를 받아서 문장 생성
    - 생성기는 안들킬만한 좋은 문장을 만들어야하고
    - cnn encoder는 안들킬만한 좋은 feature를 뽑아내는 데 집중해야 함
    - Generator loss는 기존의 생성된 단어와 정답 간의 차이를 비교하는 cross-entropy의 대체버전인 MMD를 사용했음. 
    - Discriminator loss는 3개로 이루어져있는데 좀 복잡함
        - 첫 번째는 cnn으로 encoding된 feature를 fnn에 먹여서 진짠지 아닌지 알아내는 데에서 생겨나는 GAN loss
        - 두 번째는 정답 encoding vector와 생성된 문장 encoding vector간의 distance(l2 norm 사용)
        - 세 번째는 위에서 말한 Generator loss의 반대부호값
    
결과는 스킵할래..
1. 몇 가지 느낀 점
    - 논문에서도 비교하긴 했지만 VAE를 이용한 것과 비교하려고 썼다는 느낌이 물씬
    - Generator부분은 사실 그냥 LSTM이니까 별로 특별한 건 없고..
    - LSTM에 먹여줄 feature를 어떻게 만들까인데 VAE에서는 Variational Autoencoder를 이용한거고 이 논문에서는 cnn을 이용한거고
    - 문장을 어떻게 하면 latent space상에 좋게 배치할지에 대해 생각하는 논문이라는 점에서 기존 연구들과 잘 이어진다고 생각하는데 생각해보면 여러모로 재밌게 응용해볼 수 있을 것 같긴 함
    - 예를 들어 꼭 진짜냐 가짜냐를 맞추는게 아니더라도 남을 어떻게 하면 유창하게 속일 수 있을지(이런건 데이터 만들기가 정말 어렵겠지?) 뭐 이런 것들 하는데에 이 논문 프레임워크 그대로 써먹을 수 있을 것 같은데 힣.. 모르겠다