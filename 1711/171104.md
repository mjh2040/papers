### Title
Dynamic Routing Between Capsules

### Authors
Sara Sabour, Nicholas Frosst, Geoffrey E. Hinton

### link
[Download link](https://arxiv.org/pdf/1710.09829v1.pdf)

### Contents
    - 솔직히 이 논문을 내가 제대로 다 이해했다고 생각하진 않음.. 그냥 느낀점을 이야기하면서 스토리를 한 번 풀어보려고 함
    - 인적성 검사에도 굉장히 많이 문항 중 하나인데 mental rotation 능력을 측정하는 문항들이 있음
        ![image](http://www.psytoolkit.org/lessons/rotation.png)
    - 돌려진 이미지가 원래 이미지의 어떤 부분에 해당이 되는지 맞춰가면서 풀지 않음? 내가 느낀 이 논문의 분석 방법은 딱 이런 느낌이었음
    - 제일 아랫단의 capsule은 이미지의 local representation을 나타냄
    - higher level capsule로 올라가면서 이 낮은 단계 capsule의 정보를 조합해서(조합하는데 어떤 capsule의 정보를 얼마만큼 사용해야 하는지를 계산해서 재조합) 더 의미있는 수준의 representation vector로 만들어가기
    - 그리하여 최종 capsule, high level representation의 각 원소들은 이미지의 meaningful feature를 나타낼 수 있도록 하는거지
    - capsule 재조합에서 attention(weighted sum)이 들어간다는건 내가 마음속에 표상하고 있는.. 예를 들어 고양이라고 하면 고양이의 전형적인 표상에 맞게 실제 상황에서 만나게 되는 왜곡된 이미지들로 부터 추출한 각 local feature들을 돌리고 지지고 볶고 하는 과정이라고 생각할 수 있음
    - 이렇게 계산된 마지막의 각 class별 capsule vector가 나타내는 것은 어떻게보면 지각/인지심리학에서 이야기하던 distributed representation theory에서의 brain activation과 유사하다고 볼 수 있을 것 같음, 물론 논문에서는 이 vector의 요소들이 나타내고 있는 해석가능한 의미(thickness, rotated angle, ..)에 초점을 맞췄지만 좀 더 큰 의미로 보자면 이렇게도 볼 수 있지 않을까??
    - 그런 점에서 나는 이 논문은 이미지 처리에서의 attention mechanism을 끌어왔다는 점도 좋은 부분이지만 결과적으로는 The Consciousness Prior(Bengio, 2017) 이 논문이 주장하는 바와도 연결되는 부분이 있다고 봄
    - 어떻게 이미지 / 텍스트를 high level representation으로 만들어 낼 것인가.. 혹은 그것이 가능할까와 같은 질문에 대답하는 연구가 아닐까.. 뭔가 대가끼리는 통하는 바가 있는 것 같기도 하고 모르겠다ㅋㅋㅋ
    - 구현의 디테일은 뭐 솔직히 엄청난거라곤 하기 힘들 것 같은 느낌은 있음.. 이미지 처리에서의 이런 발상을 만들어냈다는게 나는 참 좋은 점이라고 생각이 됨