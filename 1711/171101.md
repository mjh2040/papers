### Title
Massive Exploration of Neural Machine Translation Architectures

### Authors
Google Brain

### link
[Download link](https://arxiv.org/pdf/1703.03906.pdf)

### Contents
- 무지 많은 모형들 돌려서 결과 비교해보면서 benchmark용 자료 만들고 좋은 모형 구조에 대해 알아보는 논문
- rnn encoder / decoder
- attention mechanism에는 rotated hidden representation vector의 dot product로 계산
- subword unit 사용, bpe 써서 단어사전 37k 크기로 만들었음
- Experiments
    1. Embedding Dimensionality
        - 128 ~ 2048까지 늘려봤음, 전체적으로 bleu값이 좋아지긴 했지만 파라미터 수가 거의 6배 늘어나는 것에 비하면 큰 이익은 없어보임
    1. RNN Cell Variant
        - LSTM, GRU를 보통 많이 사용
        - 실험에서는 LSTM cell이 항상 좋았음
        - attention이 있으면 굳이 decoding에서 이전 시점 자료를 계속 많이 가지고 있을 필요가 없을 것이란 가정은 vanilla decoder가 그지같이 작동하는 걸로 봐서(심지어 attention mechanism 적용했는데도) 안맞는 걸로 드러나는 것 같음
    1. Encoder and Decoder Depth
        - 2층짜리 모형부터 8개층 & residual network까지 넣은거 전부 비교했는데 encoder / decoder side 모두 층이 4개 이상이어도 결고가 좋아지지 않았음, 오히려 학습이 안돼서 결과값이 그지같이 변해버림
        - 만일 층을 많이 쌓고 싶다면 optimizer / batch에 좀 더 공을 들여서 학습 과정에서 diverge를 막는 것이 중요
    1. unidirectional / bidirectional encoder
        - 대체로는 bidirectional encoder가 결과가 좋았는데 그렇다고 결과가 막 엄청 좋아진 건 또 아님
        - source는 항상 반대로 넣는 편이 결과값이 더 좋았음
    1. Attention Mechanism
        - 보통 additive, multiplicative 두 종류가 있음
        - additive attention이 조금씩 더 좋은 성능을 보였었음
        - decoding시에 encoder 정보를 이용하게 해주는 모형보다도 attention을 쓰는게 훨씬 성능이 좋음.. 일종의 weighted skip connection같은 느낌..
    1. Beam search strategies
        - beam size가 너무 커지면 성능 저하 가져옴
        - length penalty부여한 10개 beam을 쓰는게 제일 결과적으로는 좋았음
        - 좋은 결과를 얻으려면 beam search 잘 튜닝해야..
        - trainable greedy search?
- 이렇게 실험한 좋은 결과들을 기반으로 hyperparameter들도 공개하고 소스코드도 공개했음
- 흠터레스팅.. 이건 seq2seq model 기반이고 아마 encoder에 self-attention쓰느냐 아니면 cnn structure 집어넣느냐에 따라 또 달라질거임
- 모형 구조 자체의 개선이 솔직히 많이 어려워진 상황에서 이런 실험은 참 가치있는 듯..