### Title
Foolbox v0.8.0: A Python toolbox to benchmark the robustness of machine learning models

### Authors


### link
[Download link](https://arxiv.org/pdf/1707.04131.pdf)


### Contents
1. Intro
    - 자료에 조금만 perturbation 넣어도 모형 학습이 그지같이 되는 경우 있음
    - 마찬가지로 엄청나게 적은 feature의 차이임에도 불구하고 예측치는 엄청나게 달라지는 경우도 있음
    - adversarial perturbations
    - 모형이 robust하다는건 이런 feature에서의 작은 변화에도 큰 차이없이 결과값이 일정하다는거
1. Foolbox overview
    1. struture
        - model: takes an input and makes a prediction
            - tensorflow, pytorch, theano, lasagne, keras, mxnet지원
        - criterion: what an adversarial
            - misclassification
            - top-k misclassification
            - original class probability
            - targeted misclassification: 이건 특별히 확실히 지정된걸로 misclassification하는거
            - target class probability: 이것도 지정된 target으로 예측하는 확률이 일정 이상인거
        - distance measure: the size of a perturbation
            - mse
            - mae
        - attack algorithm: generate an adversarial perturbation
            - fast gradient sign method로 찾음
1. Implemented attack methods
    1. gradient-based attacks
        - 자료를 어떤 방향으로 움직였을 때에 loss가 많이 증가하는지 보고(gradient) perturbation 생성
    1. blackbox attacks
        - 자료에 특정 분포에서 나온 error 더하기 혹은 blur 먹이거나 pixel을 갈아끼워버리기
    