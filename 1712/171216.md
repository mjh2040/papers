### Title
Deep Residual Learning for Image Recognition

### Authors
Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sum

### link
[Download link](https://arxiv.org/pdf/1512.03385.pdf)

### Contents
1. Introduction
    - deep network에서 종종 일어나는 degradation
    - 오히려 layer가 적은 network보다 accuracy가 떨어지는 현상
    - 근데 deep layer중 일부만 identity function이고 나머지 layer는 shallow network의 것과 동일하다면.. 결과가 나빠질 이유가 없음
    - 못해도 같게는 나와야 하는데... 실제로는 그렇지 않다는거
    - 이 논문에서는 학습해야 하는 함수의 모양을 recast해서 학습하기 쉬운 형태로 만들어보는게 목적(residual network)

1. Related work
    - shortcut connection을 도입하고 있는 다른 모형들과 비슷한 점이 많음
    - 예를 들어 highway network는 gating function을 도입해서 layer를 거친 output과 input을 적절한 비율로 섞어서 다음 input으로 사용
    - residual network는 항상 input의 모든 정보가 전달되고 실제 layer가 학습해야 하는건 input과 실제 구현되어야 할 representation 사이의 residual

1. Deep Residual Learning
    - H(x)가 stacked layer의 mapping이라고 해보자. 원래 neural network는 이걸 배우게 해야 함
    - H(x) 대신 H(x) - x 인 F(x)를 배우게 하는게 residual network의 기본 아이디어
    - 왜 이렇게 하는걸까?
        - degradation 문제를 생각해보면... identity function을 만드는 것도 어려워보임
        - residual 형태로 만들면 identity function을 만들기 위해서는 stacked layer의 weight를 0에 가깝게만 만들면 되니 훨씬 학습해야 하는 내용이 간단함
        - 물론 실제로는 identity mapping이 아닌 다른 mapping을 학습해야 할지라도 optimal mapping이 identity mapping에 가깝다면 그만큼 학습이 쉬워짐
    - 구현도 간단한데 만일 H(x)와 x의 dimension이 같다면 그냥 elementwise addition으로 다음 input에 넣어주는 방식을 쓰면 됨
    - 만일 dimension이 다르다면 이를 맞춰주는 다른 linear projection 하나만 있으면 되고

- 결과를 보면 resnet은 layer가 많아지면 error도 떨어져서 degradation문제가 안일어났음.. 그저 갓갓
- 대충 뭔지는 봤고.. 자세한건 구현과 같이 보면 좋을 듯