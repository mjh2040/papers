### Title
Distilling a Neural Network Into a Soft Decision Tree

### Authors
Nicholas Frosst, Geoffrey Hinton

### link
[Download link](https://arxiv.org/pdf/1711.09784.pdf)

### Contents
1. Introduction
    - neural network는 중간 layer들에서 도대체 무슨 일이 벌어지고 있는지 알기 힘듦
    - decision tree에서는 무슨 일이 일어나고 있는지는 비교적 잘 알 수 있음
    - 근데 각 판단에서 적은 양의 변수만을 사용하는게 문제 -> generalization이 약함
    - 두 개를 결합해서 더 좋게 만들어보겠다는건데 이런거 비슷한거 꽤 있지 않았었나??
    
1. The Hierarchical Mixture of Bigots
    - 우선 전체 모형 구조를 보자
    - ![image](../image/171203.png)
    - 각각의 node들에는 학습된 filter w, b가 있음
    - 오른쪽 branch로 갈 확률은 p_i(x) = $sigma$(xw_i + b_i)
    - 그림에서 제일 마지막에 있는게 prob distribution임
    - 왼쪽과 오른쪽을 나눌 때 어떤 filter pattern이 사용되는지 보면 모형이 어떻게 동작하는지 잘 알 수 있을거라는거..
    - 기존의 full network의 각 layer에서 low ~ high로 올라가면서 filter들 뽑아서 봤던거랑은 좀 다른 접근법이네

1. Regularizers
    - path probability가 5:5가 되도록 맞춰주는 페널티 텀 추가
    
1. 왜 이런짓을 했나?
    - 각각의 node들의 learned filter를 보면 무슨 일이 일어나고 있는지 알 수 있음
    - 아.. 근데 이거 읽어도 뭔지 이해가 안되는게 남음
    - filter parameter도 path prob 학습시킬 때 같이 학습한다는건지
    - leaf에 있는 파라미터 $phi$도 같이 학습하는건지..
    - 이건 내가 생각하는게 맞는 것 같긴 한데 넘모 설명이 부실한 것 같다..