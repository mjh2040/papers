### Title
Listen, Attend and Spell

### Authors
William Chan, Navdeep Jaitly, Quoc V. Le, Oriol Vinyals

### link
[Download link](https://arxiv.org/pdf/1508.01211.pdf)

### Contents
    - model 구조는 seq2seq 모형 구조임
    - 음성 자료를 latent vector로 바꿔주는 encoder(Listener)와 저 정보를 이용해서 character로 바꿔주는 decoder(Attend and Speller) 두 요소로 구성됨
    - attend and speller는 그냥 별 특별한 게 없는 attention based lstm 모형임
    - input을 받아서 character로 decoding하는 모형
    - 대신 encoder 부분이 좀 특별한데 이건 이유가 있음
    - 보통의 텍스트 자료랑은 다르게 음성 자료는 무지무지 길이가 김(input length가 1000을 훌쩍 넘어가는 경우 많음)
    - 그래서 그냥 lstm에서 나오는 hidden state들을 decoder의 input으로 사용하면 attention mechanism이 작동을 잘안함
    - 여기서는 이 문제를 해결하기 위해서 piramidal structure를 도입했음
    - 뭔가 복잡해보이는 것 같지만 복잡한게 아니고 t 시점의 lstm의 다음 layer의 input을 그 전 t시점의 자료만 집어넣는게 아니고 2t-1, 2t를 concatenation해서 집어넣는거
    - layer 수만큼 그러면 차원이 exponential하게 줄어드니까 마지막 decoder를 위한 input의 길이가 조절이 됨
    - decoding 다음 language model로 rescoring하는 부분이 있는데 이게 결과를 확실히 좋게 만들어 줌
    - 아마 beam search 결과 중에 일반적으로 잘안나오는 이상한 문자열이나 단어들을 걸러주는 효과가 있기 때문인 듯
    - 논문에서 사실 제일 모르겠는 부분은 데이터 부분인 것 같은데.. mel-scaled filter bank가 아직 뭔지 확실히 감이 안옴
    - 일단 참고 [링크](http://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html)
    - frequency때문에 데이터가 이미지처럼 2차원인데.. 펴서 집어넣는 것 같지는 않고 이 부분은 코드를 좀 보면 이해가 될 것 같음
    - 그리고 training할 때 scheduled sampling 사용했음
    - 결과는 따로 안적어야지ㅎㅎ 귀찮다