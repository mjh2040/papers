### Title
Exploring the Limits of Language Modeling

### Authors
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, Yonghui Wu

### link
[Download link](https://arxiv.org/pdf/1602.02410.pdf)

### Contents
1. Introduction
    - Language Modeling 요새 연구가 많이 되고 있지만 엄청 큰 규모의 dataset에 적용되는 경우는 좀 드물었음
    - large scale LM에 대한 benchmark 만들어보려고 함

1. Related works
    1. Language models
        - p(w_1, ..., w_t)를 알아내는 모형
        - n-gram이나 log-linear model등을 쓰는 전통적 모형이 있지만 요즘에는 거의 다 rnn 구조로 학습
        - p(w_1, ..., w_t) = $prod$p(w_i | w_1, ..., w_(i-1))
    1. Convolutional Embedding Models
        - character에 대해서 bidirectional lstm으로 접근한 방식과 cnn으로 접근해서 embedding 만들어 낸 방식이 있음
        - large scale LM에서는 word 차원에서 접귾면 embedding matrix가 무지 커지기 때문에 character level로 접근해보는게 좋은 선택임
    1. Softmax Over Large Vocabularies
        - 마지막 layer에서 softmax 쓰면 partition function 구하는데에 너무 많은 자원 소모
        - Importance sampling 이용해서 계산량 줄임

1. Results
    - 결과물을 보면 Big LSTM + CNN input이 가장 결과가 좋았음(word embedding을 넣은 lstm보다 결과물이 나음)
    - cnn embedding의 뛰어남 입증?
    - regularization 중요하더라

1. 잡생각
    - 아마 text adversarial attack에서도 단순 cnn보다는 cnn embedding + rnn 구조가 좀 더 결과물이 좋을 것 같음
    - cnn embedding + rnn의 좋은 점은 rnn 구조 덕분에 본문 길이를 통제할 필요가 없어짐
