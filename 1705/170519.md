### Title
Deep Reinforcement Learning: An Overview

### Authors
Yuxi Li

### link
[arxiv](https://arxiv.org/pdf/1701.07274.pdf)

### Contents
1. Introduction
    - sequential decision making 문제 해결에 사용
    - breakthroughs: 너무 많아서 정리하기도 힘들다.. 여기 있는걸 다 하나씩 짚어준다니 논문을 읽어가면서 정리합시다
1. background
    - Neural Network
        - 간단하게 옛날 neural network부터 cnn, rnn, dropout, batch normalization 이런 이야기
    - Reinforcement Learning
        - $\pi\$(a_t|t), R(s, a), P(s_t+1|s_t, a_t) -> policy, reward, transition probability
        - value function, action-value function -> TD learning/ Q-learning or SARSA
        - 위의 function을 neural network로 근사하는게 deep Q-Network류
        - policy based methods는 policy를 바로 구하기, REINFORCE, actor-critic
    - Testbeds
        - Arcade Learning Environment, DeepMind Lab, OpenAI Gym, OpenAI Universe, FAIR TorchCraft, ViZDoom, TORCS, MuJoCo
1. Deep Q-Network
    - Mnih et al(2015) -> 아타리 게임 논문
    - Experience replay: 그냥 online learning하는게 아니고 순서를 막 섞어서 minibatch 만들어서 자료 사이의 연관성 낮추고 학습 안정화. 중요한/드문 자료는 학습에 여러번 쓸 수 있음
    - Double DQN: deep q-learning은 parameter update 과정에서 value값을 overestimate하는 경향이 있음, 두 개의 network 만들어서 하나는 greegy policy가 뭔지 찾도록 하고 나머지 하나는 value update하도록. 
    - Prioritized experience replay: experience replay에서 important experience가 더 자주 나올 수 있도록 -> td error기반으로(아마도 이게 크면 더 자주 나오도록 하겠지? 자세한건 내일 논문 읽어보고 다시 내용 덧붙이도록 하자)
    - dueling architecture: state value function과 advantage function을 예측하는 network 만들고 action value function 추정-> Q-learning보다 더 빨리 수렴(이것도 네트워크 구조 좀 더 자세히 봐야할 것 같음, 원래 q-learning에서는 action value function만을 approximation하는데 value function도 예측하게 해서 noise를 줄이는 효과가 있는 것으로 아마도 예상함)
1. 이어서 계속