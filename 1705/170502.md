### Title
Dialogue learning with human-in-the-loop

### Authors
Jiwei Li, Alexaander H. Miller, Sumit Chopra, Marc'Aurelio Ranzato, Jason weston

### link
[arxiv](http://arxiv.org/abs/1511.08130)

### Contents
- pre-trained model + 사람과의 QnA를 통해서 질문에 대한 대답 학습
- 기본적인 골격은 Weston(2016) 연구와 유사. 차이점이라면 여기서는 실제 사람 응답을 mechanical turk로 받았고 reinforcement learning 적용했다는 점
- 학습용 과제는 bAbI datasets, WikiMovies datasets
- WikiMovies Task 6: Partial Rewards
  - example
<table>
<tbody>
<tr><td>
What films are about Hawaii?<br>
<span style="color:red">50 First Dates<br></span>
Correct!<br>
Who acted in Licence to Kill?<br>
<span style="color:red">Billy Madison<br></span>
No, the answer is Timothy Dalton.<br>
What genre is Saratoga Trunk in?<br>
<span style="color:red">Drama<br></span>
Yes! (+)<br>
</td></tr>
</tbody>
</table>

- Methods
  - model architecture: variants of the End-to-End Memory Network(MemN2N, Sukhbaatar et al., 2015)
  - reinforcement learning
    - state: the dialogue history
    - action space: the set of answers the MemN2N has to choose from to answer the teacher's question
    - algorithms: reward-based imitation, reinfoece, forward prediction
    - forward prediction은 numerical reward가 주어지지 않았을 경우 teacher's feedback 유추하는 neural network 만들어서 이를 이용해서 rewarding
    
### 느낀점
- text generation + reinforcement learning 이 조합은 올 한해도 유행일 듯
- reward 만드는 tf algorithm, qna과제에서는 유용할 듯
- 일상 생활 대화 생성시 어떻게 reward를 만들어 낼 것인가는 역시 계속 문제가 될 듯, 어떤 응답이 좋은 응답일지에 대한 고찰이 필요함