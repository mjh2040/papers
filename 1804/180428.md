### Title
Sequence Transduction with Recurrent Neural Networks

### Authors


### link
[Download link](https://arxiv.org/pdf/1702.07463.pdf)

### Contents
- 이 논문은 두 sequences X = {x1, ..., xn}, Y = {y1, ..., ym} 이 두개를 align하는 문제에 관한 논문임
- text-to-speech나 speech recognition같은 분야에서는 일반적으로 inp - out vector의 길이가 다름
- 옛날에는 그래서 alignment를 다 맞춰줘야 했었음...
- 여기서는 input sequence를 처리하는 transcription network와 output을 예측하는 prediction network를 이용해서 두 sequence의 길이가 달라도 매 timestep마다 필요한 x를 꺼내쓰는걸 학습함
- 중요한 도입 포인트는 null output의 도입임
- 이 null output은 alignment의 경계를 그어주는 역할을 담당하게 됨
- y에서 null이 나오면 x의 timestep을 다음으로 넘김
- input이 output보다 긴 상황에 대해서(n=1:4, m=1:3)
    - {y1, null, null, y2, null, y3} 이렇게 길이를 맞춰줄 수 있고
- input이 output보다 짧은 상황에서는(n=1:3, m: 1:5)
    - {y1, y2, y3, null, y4, null, y5} 이렇게 하나의 x에서 여러개를 뽑을 수 있음
    
- p(y_t+1 | x_1:u, y_1:t) = exp(f_u, g_t) / Z로 계산되고 f_u는 transcription network의 u번째 hidden value, g_t는 prediction network의 hidden value

- 간단한 예제 코드.. 아마 이런 식이어야 할 것 같음
```
_, f = bidirectional_rnn(x)
index = 0
yindex = 0
xindex = 0

ps = []

while True:
    h = activation(W_y * y[yindex] + W_x * f[xindex] + b)
    g = Wh + b
    p(y|yindex, xindex) = exp(f[xindex] + g)
    if argmax(p) != null:
        yindex += 1
    else:
        xindex += 1
    ps.append(p)
    if yindex == len(y):
        break
    
```
- 저기서 구해지는 확률은 p(Y | X)는 아님... 그럼 이건 어떻게 구하냐?
- forward-backward algorithm을 이용함, 그래서 argmax의 prob만 저장하면 안되고 p(y=null|t, u)도 남겨야 함
- objective function은 log(p(Y|X)), 이렇게 하면 이제 x와 y의 alignment를 만들어주는 모형 prediction network를 학습할 수 있음
- beam search 부분은 좀만 있다가 다시 살펴보자...