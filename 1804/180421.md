- Distributed SGD
    - neural network의 distributed training에는 크게 두 가지 방법이 있음
        1. Model Parallelism
        1. Data Parallelism
    - Model Parallelism은 큰 network를 여러개 device에 나눠서 update하는 방식
        - 만일 이렇게 한다면 하나 gpu memory에는 다 못담는 큰 network를 만드는 것이 가능
        - 모형이 복잡해지니 훨씬 다양하고 많은 데이터를 잘 예측할 수 있는 능력을 가질 수 있겠지
        - 근데 현실적으로는 잘 안씀
        - 일단 device간 node 계산값 넘겨주고 이렇게 하는게 latency가 크기도 하고 요즘 그래픽카드는 메모리 11g는 되는데 이정도면 어지간히 큰 모형은 다 하나의 device에서 띄울 수가 있음
        - 게다가 저렇게 모형을 잘 나눠서 장치에 분배하려면 그에 맞게 모형을 설계해야 하는데... 그럴바에야 hypernetwork같은 걸 써서 parameter 줄이는 노력을 하는게 나음...
    - Data Parallelism은 이에 비하면 엄청 심플함
        - 이것도 synchronous / asynchronous 두 방법으로 나눠볼 수 있음
            1. synchronous
                - synchronous는 이름에서도 알 수 있듯... 여러 장치가 동시에 parameter update하는거임
                - device별로 똑같은 모형들을 띄워놓고 서로 다른 batch를 주고 loss, gradient 계산함
                - 모든 device에서 gradient 계산하면 이걸 다 합쳐서(average, ...) 하나의 normalized gradient를 만들어 냄
                - 그러면 모든 장치가 이 normalized gradient를 가지고 parameter update를 하는거
                - 일반적으로 neural network에서 batch를 키우는게 모형 학습에 좋은 면이 있는데 이렇게 하면 하나의 장치에서 큰 batch data를 보는 것과 비슷한 효과를 낼 수 있음
                - 그래서 좀 더 빨리 수렴하게 만들 수 있는거
                - 대신 이렇게 하면 하나의 device라도 망하거나 속도가 늦어지면 전체 update과정이 속도가 늦어짐
        
            1. asynchronous
                - 이건 global model parameter server를 따로 두고 replica들이 여기로 update 정보 보내주는 식임
                - 왜 asynchronous냐면 각 device는 서로간의 결과를 공유하지 않음
                - 그냥 parameter server하고만 gradient, parameter 정보 주고받을 뿐
                - 그래서 다른 모델에서 속도가 늦든 안늦든 아무 문제가 없음
                - 이렇게 생각하면 좋은 점만 있는 것 같은데 문제도 있음
                - stale parameter update라고 하는건데... device_0에서 w를 기반으로 gradient를 보내줬다고 해보자. 근데 이미 model server에는 device_1이 보내준 gradient로 update를 해서 w가 아닌 w' 상태임
                - 이러면 이전 parameter에 기반해서 parameter update를 하게 되는거고 결과적으로 성능이 떨어질 수 있음
                - 그래서 parameter server와의 통신을 좀 덜하게 만든다든지 여러 방법을 사용함
        - 보통 예제에서 많이 나오는건 synchronous한 방법이고... 구현도 무지 심플간단함
        - 각 device마다 loss 구해서 tower loss, average gradient만 만들어주면 됨
