### Title
Enhanced Neural Machine Translation by Learning from Draft

### Authors
Aodong Li, Shiyue Zhang, Dong Wang and Thomas Fang Zheng

### link
[Download link](https://arxiv.org/pdf/1710.01789.pdf)

### Contents
1. Introduction
    - 기존 NMT system에서 문제될 수 있는 점 하나는 decoding시 left decoding history만을 사용한다는 점
    - 어떻게 보면 당연한거지만 만일 내가 t시점의 단어를 예측할 때에 그 전만이 아닌 그 다음에 나올 단어들도 알고 있다면 번역이 더 좋아질 것
    - two-stage translation을 이용해서 decoding시 right context information도 이용할 수 있도록 해보겠당
    
1. Translation by Learning from Draft
    - 아이디어 자체는 간단함
    - 기존 NMT 모형가지고 우선 번역 draft 만들고
    - source word와 draft를 다시 각각 encoding해서
    - 각자 attention 적용하고
    - 두 개의 context vector를 합쳐서(concatenation)
    - 하나의 context vector로 만들어서 decoding시 사용하는거

1. Experiment and result
    - NIST, IWSLT dataset(cn -> en)
    - moses, single attention model과 비교했음
    - moses와는 competitive, single attention model보다는 bleu점수 확실히 좋아짐
    - 간단하지만 쉽게 적용은 해볼 수 있는 모형인 것 같긴 함. 근데 parameter수가 지나치게 많아질 것 같은데 이거 적용할 때 draft encoding하는 부분이 쓸데없이 커지는건 아닐까..
    - 모델 2개 가지고 하는 거에 비해서 효율이 얼마나 좋을지.. 하나의 큰 모형보다 과연 결과가 좋을까?