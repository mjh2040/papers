### Title
Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation

### Authors
넘많다.. 걍 구글팀

### link
[Download link](https://arxiv.org/pdf/1611.04558.pdf)

### Contents
1. Introduction
    - single language pair.. 다양한 언어 pairs를 만들어 낼 방법이 없으면 좋은 NMT model 못만들어..
    - 하나의 모형으로 다양한 언어 번역하는 system 만들기
    - target artificial token을 input에 집어넣는 것만으로도 다언어 지원이 가능한 식으로 마개조함
    - 이런 구조의 장점
        - 간단, 모형도 적게 필요하고 학습 시간도 적어도 되고 학습도 걍 자료 다 섞어놓고 하면 되고
        - 적은 자료의 번역질이 향상
        - 한 번도 조합에 없었던 언어 조합도 번역할 수 있음(zero shot learning)
        
1. System Architecture for Multilingual Translation
    - 구조는 원래 구글 번역기 구조와 거의 똑같음
    - input sentence 제일 처음에 특수 토큰 추가  
    <table>
    <tr>
        <td>&lt;2es&gt; Hello, how are you? -> Hola, ¿cómo estás?</td>
    </tr>
    </table>
    - 저 &lt;2es&gt; 표시가 to spanish 토큰임, 그러면 output language가 spanish라는걸 알 수 있도록
    - 모든 언어 pairs를 한 번에 넣고 돌려서 vocab size 제한할 필요가 있음
    - shared wordpiece model 사용했다고 함
        - 대충 어떤 개념인지만 말하자면.. 나는 맛있는 밥을 먹었다와 같은 문장이 있으면 맛있는 밥이라는 표현이 많이 나올 때 이를 하나의 token으로 보는거임. 띄어쓰기로 다 구분해서 맛있는과 밥, 을, 이런게 개별 토큰이 되는게 아니고

1. Experiments and Results
    - 다양한 방법으로 실험해봄.. 역시 갓구글
        - many source languages to one target language
        - one source language to many target languages
        - many source languages to ,any target languages
    - 데이터 양 조절할 때 oversampling 사용했다고 했는데.. character embedding 기반이라면 perturbation도 같이 이용할 수 있지 않을까?
    - Many to One
        - 각각을 따로 학습한 것이 baseline model, source쪽 언어를 섞어서 하나의 모형을 학습한 것이 비교할 모형
        - 놀랍게도 영어로 번역한 6종류의 언어(프, 독, 한, 일, 스, 포)에서 single model보다 성능 향상
    - One to Many
        - 여기서는 모든 언어에서 성능향상 나온건 아님,, 영어ㄹ를 불어랑 한국어로 번역하는 것은 성능이 조금 떨어짐
        - oversampling하면 적은 수 데이터의 언어 번역은 성능 좀 올라가지만 대신 majority data 언어는 성능 떨어짐
        - shared wordpiece가 언어끼리 섞여있어서 그럴 가능성이 좀 높다고 저자들은 분석했음
    - Many to Many
        - 성능은 좀 많이 떨어짐
        - 대신 하나의 모형으로 여러개 언어 번역할 수 있으니 나름 이득은 있는셈
    - Large-scale Experiments
        - 파라미터 수를 확 늘려서 multilingual model 학습시켜봄
        - 결과는 조금 더 좋아졌음. single model에 비해 -2.5%, 그래도 여전히 파라미터 수는 single models 다 합친 것보다 1/12수준
    - Zero-shot Translation
        - implicit bridging이 일어남
        - 원래는 학습자료에 없던 조합은 중간에 다른 언어를 거쳐서 번역해야 하는데 multilingual model은 이런걸 내부적으로 가지고 있는 모습..
        - 심지어 bleu값도 single model explicit bridging한 것이랑 비교해보면 꽤 양호한 수준임
        - 제로샷 모형에서 target pair들을 조금만 더 학습시켜줘도 번역 퀄리티가 거의 처음부터 열심히 학습시킨 것 수준으로 올라감

1. Mixing language
    - 두 가지 실험했음
        - input language가 두 언어가 섞여있어도 번역이 될까? 번역이 됨
        - output token이 2개 있으면 어떻게 될까? weight에 따라서 두 언어가 섞여서 번역이 됨
        - 개신기..
